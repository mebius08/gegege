{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =============================================================================\n# ШАБЛОН №2: LoRA + RAG через LangChain (красиво, цепочками, но всё ещё быстро и без OOM)\n# =============================================================================\nfrom datasets import Dataset\nfrom unsloth import FastLanguageModel\nimport torch\nfrom transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\n\nfrom tqdm.auto import tqdm\nimport pandas as pd\nimport gc\nimport os\n\n# =============================================================================\n# 1. НАСТРОЙКИ — всё тоже самое\n# =============================================================================\nOUTPUT_DIR = \"./qwen3-14b-langchain-rag\"\nMODEL_NAME = \"unsloth/Qwen3-14B-unsloth-bnb-4bit\"\nMAX_SEQ_LENGTH = 8192\n\nTRAIN_PATH = \"/kaggle/input/your-dataset/train.csv\"\nTEST_PATH  = \"/kaggle/input/your-dataset/test.csv\"\nTARGET_COLUMN = \"target\"\nK_RETRIEVAL = 5                     # сколько примеров тащим\n\n# =============================================================================\n# 2. Модель + LoRA (то же самое, похуй)\n# =============================================================================\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = MODEL_NAME,\n    max_seq_length = MAX_SEQ_LENGTH,\n    dtype = torch.bfloat16,\n    load_in_4bit = True,\n    use_gradient_checkpointing = \"unsloth\",\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 64,\n    lora_alpha = 128,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout = 0.05,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 42,\n)\n\n# =============================================================================\n# 3. Данные\n# =============================================================================\ntrain_df = pd.read_csv(TRAIN_PATH)\ntest_df  = pd.read_csv(TEST_PATH) if TEST_PATH and os.path.exists(TEST_PATH) else None\n\ndef row_to_text(row, include_target=False):\n    items = []\n    for col, val in row.items():\n        if col == TARGET_COLUMN and not include_target:\n            continue\n        val = f\"{val:.6f}\".rstrip(\"0\").rstrip(\".\") if isinstance(val, float) else str(val)\n        items.append(f\"{col}: {val}\")\n    return \", \".join(items)\n\n# =============================================================================\n# 4. LangChain RAG — вот ради чего всё это\n# =============================================================================\nprint(\"Греем эмбеддер и строим FAISS индекс (5-10 минут на 100k строк)...\")\nembedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Делаем документы для LangChain (page_content = строка, metadata = таргет для дебага)\ndocuments = []\nfor idx, row in tqdm(train_df.iterrows(), total=len(train_df)):\n    text = row_to_text(row, include_target=False)\n    target = row[TARGET_COLUMN]\n    doc = Document(\n        page_content=text,\n        metadata={\"target\": target, \"row_id\": idx}\n    )\n    documents.append(doc)\n\n# FAISS в памяти — быстро и без записи на диск (на 200k строк ~6-8 ГБ RAM, норм)\nvectorstore = FAISS.from_documents(documents, embedding)\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": K_RETRIEVAL})\n\n# Красивый промпт через LangChain\ntemplate = \"\"\"<|system|>\nYou are a helpful assistant. Predict the target using the current row and k nearest examples from training data.\n<|end|>\n<|user|>\nNearest examples:\n{context}\n\nCurrent row:\n{question}\n\nPredict only the number (or class), no explanations.\n<|end|>\n<|assistant|>\n\"\"\"\n\nprompt = ChatPromptTemplate.from_template(template)\n\n# Цепочка (просто для красоты, на инференсе будем вызывать вручную)\nrag_chain = (\n    {\"context\": retriever | (lambda docs: \"\\n\".join([f\"- {d.page_content} → target: {d.metadata['target']}\" for d in docs])),\n     \"question\": RunnablePassthrough()}\n    | prompt\n    | StrOutputParser()   # пока не используем, но можно\n)\n\n# =============================================================================\n# 5. Форматирование датасета с LangChain-ретривером (батчами, без OOM)\n# =============================================================================\ndef apply_langchain_rag(examples, is_train=True):\n    prompts = []\n    labels = [] if is_train else None\n\n    for i in range(len(examples[\"__index__\"])):\n        idx = examples[\"__index__\"][i]\n        row = train_df.iloc[idx] if is_train else test_df.iloc[idx]\n        query = row_to_text(row, include_target=False)\n\n        # ←←← ВОТ ЭТО САМОЕ КРАСИВОЕ МЕСТО — LangChain ретривер\n        retrieved_docs = retriever.invoke(query)\n        context = \"\\n\".join([f\"- {doc.page_content} → target: {doc.metadata['target']}\" \n                            for doc in retrieved_docs])\n\n        full_prompt = f\"\"\"<|system|>\nYou are a helpful assistant. Predict the target using features and nearest examples.\n<|end|>\n<|user|>\nNearest examples:\n{context}\n\nCurrent row:\n{query}\nPredict only the number.\n<|end|>\n<|assistant|>\n\"\"\"\n        prompts.append(full_prompt)\n\n        if is_train:\n            labels.append(str(row[TARGET_COLUMN]))\n\n    # Токенизация батчем — быстро и без боли\n    tokenized = tokenizer(\n        prompts,\n        padding=False,\n        truncation=True,\n        max_length=MAX_SEQ_LENGTH,\n    )\n\n    result = {\n        \"input_ids\": tokenized[\"input_ids\"],\n        \"attention_mask\": tokenized[\"attention_mask\"],\n    }\n\n    if is_train:\n        tok_labels = tokenizer(labels, padding=False, truncation=True, max_length=256)\n        full_labels = []\n        for i, inp_ids in enumerate(tokenized[\"input_ids\"]):\n            label = [-100] * len(inp_ids) + tok_labels[\"input_ids\"][i]\n            label = label[:MAX_SEQ_LENGTH] + [-100] * (MAX_SEQ_LENGTH - len(label))\n            full_labels.append(label)\n        result[\"labels\"] = full_labels\n\n    return result\n\n# =============================================================================\n# 6. Датасеты\n# =============================================================================\ntrain_ds = Dataset.from_pandas(train_df.reset_index().rename(columns={\"index\": \"__index__\"}))\ntest_ds  = Dataset.from_pandas(test_df.reset_index().rename(columns={\"index\": \"__index__\"})) if test_df is not None else None\n\nprint(\"Форматируем трейн через LangChain RAG...\")\ntrain_formatted = train_ds.map(\n    lambda x: apply_langchain_rag(x, is_train=True),\n    batched=True,\n    batch_size=16,\n    remove_columns=train_ds.column_names,\n)\n\nif test_ds is not None:\n    test_formatted = test_ds.map(\n        lambda x: apply_langchain_rag(x, is_train=False),\n        batched=True,\n        batch_size=16,\n        remove_columns=test_ds.column_names,\n    )\n\nsplit = train_formatted.train_test_split(test_size=0.1, seed=42)\ntrain_dataset = split[\"train\"]\nval_dataset = split[\"test\"]\n\n# =============================================================================\n# 7. Тренировка (то же самое)\n# =============================================================================\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    logging_steps=10,\n    eval_steps=200,\n    save_steps=500,\n    warmup_steps=50,\n    fp16=True,\n    optim=\"paged_adamw_8bit\",\n    report_to=\"none\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n)\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, padding=\"longest\", pad_to_multiple_of=8)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\n\ntrainer.train()\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\n\n# =============================================================================\n# 8. Инференс (тоже через LangChain ретривер)\n# =============================================================================\nFastLanguageModel.for_inference(model)\n\n@torch.no_grad()\ndef predict_langchain(batch):\n    input_ids = torch.tensor(batch[\"input_ids\"]).to(model.device)\n    attn_mask = torch.tensor(batch[\"attention_mask\"]).to(model.device)\n\n    generated = model.generate(\n        input_ids=input_ids,\n        attention_mask=attn_mask,\n        max_new_tokens=64,\n        temperature=0.0,\n        do_sample=False,\n        pad_token_id=tokenizer.eos_token_id,\n    )\n\n    texts = tokenizer.batch_decode(generated, skip_special_tokens=False)\n    preds = []\n    for t in texts:\n        pred = t.split(\"<|assistant|>\")[-1].split(\"<|end|>\")[0].strip()\n        try:\n            pred = float(pred.replace(\",\", \"\"))\n        except:\n            pass\n        preds.append(pred)\n    return {\"prediction\": preds}\n\nif test_ds is not None:\n    preds = test_formatted.map(predict_langchain, batched=True, batch_size=8)\n    submission = pd.DataFrame({TARGET_COLUMN: preds[\"prediction\"]})\n    submission.to_csv(\"submission_langchain.csv\", index=False)\n    print(\"Готово, бери submission_langchain.csv и вали на лидерборд!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}