### Что делать с такой переменной делать?

Ты нашёл признак, который **очень сильно разделяет train и test** (например, по нему можно почти идеально предсказать, в каком сете наблюдение находится), и при этом он **сильно коррелирует с таргетом**. Это классический **data leakage** или близкий к нему случай, и почти всегда такая переменная опасна.

Вот что обычно делают в зависимости от природы переменной:

#### 1. Если переменная — это прямой или косвенный leakage (самый частый случай)
Примеры:
- ID транзакции/заявки/пользователя, который генерировался позже даты отсечки теста
- Дата/временная метка, которая отличается в train и test
- Признак типа «модель_2024_v2», который есть только в тесте
- Баланс на конец периода, когда таргет — дефолт в следующие 3 месяца и т.д.

**Решение: полностью удалить из модели и из всех препроцессингов**.  
Даже если по ней модель даёт +0.15 AUC на валидации — на приватном LB или в проде это почти гарантированно упадёт в ноль или в минус.

#### 2. Если переменная технически легитимная, но сильно различается по распределению между train/test
Примеры:
- Признак «регион», который в тесте имеет новые значения
- Признак «тип продукта», который появился только после периода train
- Любая категориальная переменная с сильным shift’ом

**Варианты действий (по убыванию предпочтительности):**
1. Удалить признак полностью (самый безопасный вариант).
2. Сделать target encoding / frequency encoding / WOE только на train, а новые категории в тесте закодировать как отдельную («new») или средним/медианой.
3. Использовать только те категории, которые есть и в train, и в test (обрезать редкие/новые).
4. Добавить флаг «is_new_category» и закодировать новые как 0 или среднее.

#### 3. Если переменная — это реальный сильный бизнес-признак, но он «слишком хорош»
Бывает редко, но случается (например, «был ли клиент в черном списке на момент скоринга», когда таргет — дефолт через 12 мес).

**Что делать:**
- Проверить, действительно ли этот признак будет доступен в момент предсказания в продакшене.
- Если да — можно оставить, но:
  - Обязательно делать кросс-валидацию по времени (Purged KFold, чтобы имитировать реальное разделение).
  - Смотреть на стабильность коэффициента/важности на отложенной выборке или на LB private.
  - Часто такие признаки дают overfit по времени, даже если нет прямого leakage.

#### 4. Диагностика: как точно понять, что это leakage

```python
# Быстрая проверка
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score

# модель только на этом одном признаке
auc_single = roc_auc_score(y_train, train[bad_feature])  # часто > 0.9–0.99

# модель, которая предсказывает, в каком сете наблюдение (train=0, test=1)
y_set = [0]*len(train) + [1]*len(test)
X_set = pd.concat([train[[bad_feature]], test[[bad_feature]]])
auc_set = roc_auc_score(y_set, X_set[bad_feature])  # если > 0.85–0.9 → почти наверняка leakage
```

Если второй AUC > 0.8 — удаляй без сожалений.

#### Краткий чек-лист (что делать:

| Ситуация                                      | Решение                          |
|-----------------------------------------------|----------------------------------|
| Прямой leakage (дата, ID, будущая инфа)       | Удалить полностью                |
| Категория с новыми значениями в тесте         | Удалить или обработать новые     |
| Сильный легитимный признак, доступный в проде | Оставить + строгая CV по времени |
| AUC по одному признаку > 0.95 и таргет бинарный | 99 % — leakage → удалить         |

Короче: 99 % случаев — просто удаляй. Лучше потерять 0.02–0.05 на публичном LB, чем обвалиться на привате или в продакшене.
