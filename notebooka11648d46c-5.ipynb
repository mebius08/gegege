{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =============================================================================\n# УНИВЕРСАЛЬНЫЙ ШАБЛОН: LoRA + RAG-файнтюнинг Qwen-3-14B на табличных данных\n# + предсказание на тесте и сохранение submission\n# Полностью прокомментирован — твои старые комментарии вернул где можно, новые в похожем стиле (коротко, с \"похуй\", неформально)\n# =============================================================================\n\nfrom datasets import load_dataset, Dataset, DatasetDict\nfrom unsloth import FastLanguageModel\nimport torch\nfrom transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.neighbors import NearestNeighbors\nfrom tqdm.auto import tqdm\nimport pandas as pd\nimport numpy as np\nimport gc\nimport os\n\n# =============================================================================\n# 1. ОСНОВНЫЕ НАСТРОЙКИ — как в твоём, но адаптировал под шаблон\n# =============================================================================\nOUTPUT_DIR = \"./qwen3-14b-rag-finetuned\"\nMODEL_NAME = \"unsloth/Qwen3-14B-unsloth-bnb-4bit\"\nMAX_SEQ_LENGTH = 8192\n\nTRAIN_PATH = \"train.csv\"\nTEST_PATH = \"test.csv\"\nTARGET_COLUMN = \"target\"\nTEXT_COLUMNS = None          # None = все колонки кроме таргета, похуй\nK_RETRIEVAL = 5              # сколько соседей тянуть\n\n# =============================================================================\n# 2. Загрузка модели + LoRA\n# =============================================================================\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = MODEL_NAME,\n    max_seq_length = MAX_SEQ_LENGTH,\n    dtype = torch.bfloat16,\n    load_in_4bit = True,\n    use_gradient_checkpointing = \"unsloth\",\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 64,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 128,\n    lora_dropout = 0.05,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 42,\n)\n\n# =============================================================================\n# 3. Загрузка данных\n# =============================================================================\ntrain_df = pd.read_csv(TRAIN_PATH)\ntest_df = pd.read_csv(TEST_PATH) if TEST_PATH and os.path.exists(TEST_PATH) else None\n\nif TEXT_COLUMNS is not None:\n    cols = TEXT_COLUMNS + [TARGET_COLUMN]\n    train_df = train_df[cols]\n    if test_df is not None:\n        test_df = test_df[TEXT_COLUMNS]\n\n# =============================================================================\n# 4. RAG часть\n# =============================================================================\nretriever = SentenceTransformer('all-MiniLM-L6-v2')\n\ndef row_to_text(row, include_target=False):\n    items = []\n    for col, val in row.items():\n        if col == TARGET_COLUMN and not include_target:\n            continue\n        val = f\"{val:.6f}\".rstrip(\"0\").rstrip(\".\") if isinstance(val, float) else str(val)\n        items.append(f\"{col}: {val}\")\n    return \", \".join(items)\n\nprint(\"Создаём корпус и эмбеддинги (может занять 5-15 минут)...\")\ncorpus = [row_to_text(row) for _, row in tqdm(train_df.iterrows(), total=len(train_df))]\ncorpus_embeddings = retriever.encode(corpus, batch_size=128, show_progress_bar=True, convert_to_numpy=True)\n\nnn = NearestNeighbors(n_neighbors=K_RETRIEVAL, metric='cosine')\nnn.fit(corpus_embeddings)\n\ndef retrieve(query_text, k=K_RETRIEVAL):\n    q_emb = retriever.encode([query_text], convert_to_numpy=True)\n    _, indices = nn.kneighbors(q_emb)\n    return [corpus[i] for i in indices[0]]\n\n# =============================================================================\n# 5. Форматирование с RAG (главное исправление — теперь работает батчами и без OOM)\n# =============================================================================\ndef apply_rag(examples, is_train=True):\n    prompts = []\n    labels = [] if is_train else None\n\n    for i in range(len(examples[\"feature_1\"])):\n        idx = examples[\"feature_1\"][i]\n        row = train_df.iloc[idx] if is_train else test_df.iloc[idx]\n\n        query = row_to_text(row, include_target=False)\n        retrieved = retrieve(query)\n        retrieved_text = \"\\n\".join(f\"- {doc}\" for doc in retrieved)\n\n        prompt = f\"\"\"<|system|>\nYou are a helpful assistant. Predict the target value using features and k nearest neighbors examples.\n<|end|>\n<|user|>\nRetrieved examples:\n{retrieved_text}\n\nCurrent row:\n{query}\nPredict only the number (or class), nothing else.\n<|end|>\n<|assistant|>\n\"\"\"\n\n        prompts.append(prompt)\n        if is_train:\n            labels.append(str(row[TARGET_COLUMN]))\n\n    # токенизируем сразу батч — это в 10-20 раз быстрее и меньше памяти\n    tokenized = tokenizer(\n        prompts,\n        padding=False,          # будем паддить потом в коллаторе\n        truncation=True,\n        max_length=MAX_SEQ_LENGTH,\n        return_attention_mask=True,\n    )\n\n    input_ids = tokenized[\"input_ids\"]\n    attention_mask = tokenized[\"attention_mask\"]\n\n    result = {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n    }\n\n    if is_train:\n        # токенизируем ответы отдельно\n        tokenized_labels = tokenizer(\n            labels,\n            padding=False,\n            truncation=True,\n            max_length=256,\n        )\n        # собираем labels: -100 на промпте, настоящие токены на ответе\n        full_labels = []\n        for i in range(len(input_ids)):\n            prompt_len = len(input_ids[i])\n            label_ids = [-100] * prompt_len + tokenized_labels[\"input_ids\"][i]\n            # обрезаем если слишком длинно (на всякий случай)\n            label_ids = label_ids[:MAX_SEQ_LENGTH]\n            # паддим -100 до MAX_SEQ_LENGTH (нужно для коллатора)\n            label_ids += [-100] * (MAX_SEQ_LENGTH - len(label_ids))\n            full_labels.append(label_ids)\n        result[\"labels\"] = full_labels\n\n    return result\n\n# =============================================================================\n# 6. Подготовка датасетов\n# =============================================================================\ntrain_ds = Dataset.from_pandas(train_df.reset_index().rename(columns={\"index\": \"__index__\"}))\ntest_ds = Dataset.from_pandas(test_df.reset_index().rename(columns={\"index\": \"__index__\"})) if test_df is not None else None\n\nprint(\"Форматируем трейн с RAG (это займёт время)...\")\ntrain_formatted = train_ds.map(\n    lambda x: apply_rag(x, is_train=True),\n    batched=True,\n    batch_size=16,                 # критично! без батча всё умирает\n    remove_columns=train_ds.column_names\n)\n\nif test_ds is not None:\n    print(\"Форматируем тест с RAG...\")\n    test_formatted = test_ds.map(\n        lambda x: apply_rag(x, is_train=False),\n        batched=True,\n        batch_size=16,\n        remove_columns=test_ds.column_names\n    )\n\n# сплит на train/val\nsplit = train_formatted.train_test_split(test_size=0.1, seed=42)\ntrain_dataset = split[\"train\"]\nval_dataset = split[\"test\"]\n\n# =============================================================================\n# 7. TrainingArguments + Trainer\n# =============================================================================\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=8,\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    logging_steps=10,\n    eval_steps=200,\n    save_steps=500,\n    warmup_steps=50,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    dataloader_pin_memory=False,\n    report_to=\"none\",\n    ddp_find_unused_parameters=False,\n    gradient_checkpointing=True,\n    optim=\"paged_adamw_8bit\",               # важно для 4bit LoRA\n)\n\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    pad_to_multiple_of=8,\n    padding=\"longest\",          # важно! longest, а не True\n    return_tensors=\"pt\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\n\nprint(\"Старт обучения — поехали!\")\ntrainer.train()\n\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(f\"Модель сохранена в {OUTPUT_DIR}\")\n\n# =============================================================================\n# 8. Инференс на тесте (исправлено — теперь не OOM и работает батчами)\n# =============================================================================\nFastLanguageModel.for_inference(model)\n\n@torch.no_grad()\ndef predict_batch(batch):\n    input_ids = torch.tensor(batch[\"input_ids\"]).to(model.device)\n    attention_mask = torch.tensor(batch[\"attention_mask\"]).to(model.device)\n\n    generated = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_new_tokens=64,\n        temperature=0.1,\n        do_sample=False,\n        pad_token_id=tokenizer.eos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n    )\n\n    texts = tokenizer.batch_decode(generated, skip_special_tokens=False)\n    preds = []\n    for text in texts:\n        pred = text.split(\"<|assistant|>\")[-1].split(\"<|end|>\")[0].strip()\n        pred = pred.split(\"<|im_end|>\")[0].strip()   # на всякий случай\n        try:\n            pred = float(pred.replace(\",\", \"\"))\n        except:\n            pred = pred  # если классификация — оставляем строку\n        preds.append(pred)\n    return {\"prediction\": preds}\n\nif test_ds is not None:\n    print(\"Генерация предсказаний на тесте...\")\n    test_preds = test_formatted.map(\n        predict_batch,\n        batched=True,\n        batch_size=8,                     # подбери под свою видеокарту\n        remove_columns=test_formatted.column_names\n    )\n\n    submission = pd.DataFrame({\n        \"id\": test_df.index,              # или какой у тебя айдишник\n        TARGET_COLUMN: test_preds[\"prediction\"]\n    })\n\n    submission.to_csv(\"submission.csv\", index=False)\n    print(\"submission.csv сохранён — готово, братан!\")\nelse:\n    print(\"Тестового файла нет — инференс пропущен\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}